# ============================================================================
# Face Detection 輕量級臉部偵測專案 — 訓練配置 (Ubuntu/WSL)
# ============================================================================
#
# 專案功能:
#   本專案是一套完整的臉部偵測 pipeline，涵蓋從資料標註、模型訓練、
#   推理測試到模型導出的所有環節。模型架構基於 MobileNetV2 + FPN，
#   專為邊緣裝置（手機、瀏覽器）設計，同時輸出臉部邊界框與 5 點關鍵點。
#
# 可執行腳本:
#
#   1. 標籤生成 — data/generate_labels.py
#      用 MediaPipe 自動產生 PURE 資料集的臉部標籤（bbox + 5 landmarks）
#      > python3 data/generate_labels.py --pure_dir /mnt/d/PURE --output_dir /mnt/d/PURE_labels
#
#   2. 模型訓練 — train.py
#      以 PURE 資料集訓練臉部偵測模型，支援斷點續訓、early stopping、
#      學習率排程（warmup + cosine decay）、資料增強
#      > python3 train.py --config configs/config_ubuntu.yaml
#      > python3 train.py --config configs/config_ubuntu.yaml --resume checkpoints/best_model.weights.h5
#
#   3. 單筆推理 — inference.py
#      對單張影像或影片執行推理，繪製 bbox、landmarks、confidence
#      > python3 inference.py --model checkpoints/best_model.weights.h5 --config configs/config_ubuntu.yaml --input test.jpg
#      > python3 inference.py --model checkpoints/best_model.weights.h5 --config configs/config_ubuntu.yaml --input video.avi --no-show
#
#   4. UBFC 測試集預處理 — scripts/preprocess_ubfc.py
#      將 UBFC-rPPG 資料集的 raw AVI 轉為 MJPEG 格式（OpenCV 相容）
#      > python3 scripts/preprocess_ubfc.py --ubfc_dir /mnt/d/UBFC/DATASET_2 --output_dir /mnt/d/UBFC_processed
#
#   5. UBFC 批次推理 — scripts/batch_inference_ubfc.py
#      對全部 UBFC subjects 執行推理，輸出標註影片與統計 JSON
#      > python3 scripts/batch_inference_ubfc.py --ubfc_dir /mnt/d/UBFC_processed --model checkpoints/best_model.weights.h5 --config configs/config_ubuntu.yaml --output_dir results/ubfc
#
#   6. 模型導出 — export/export_tflite.py, export/export_tfjs.py
#      將訓練好的模型轉為 TFLite（手機端）或 TensorFlow.js（網頁端）
#      > python3 export/export_tflite.py --model checkpoints/best_model.weights.h5 --config configs/config_ubuntu.yaml --output export/face_detector.tflite
#      > python3 export/export_tfjs.py --model checkpoints/best_model.weights.h5 --config configs/config_ubuntu.yaml --output export/tfjs_model
#
# 模型輸出:
#   - bbox [B, 4]       — 臉部邊界框 (x_min, y_min, x_max, y_max)，正規化 0~1
#   - landmarks [B, 5, 2] — 左眼、右眼、鼻尖、左嘴角、右嘴角
#   - confidence [B, 1]  — 臉部存在信心分數
#
# 硬體限制 (i5-12400F / RTX 4060 Ti 16GB / 32GB RAM):
#   - GPU 記憶體上限 80% (~13GB)
#   - CPU 線程上限 80% (~10 threads)
#   - batch_size 設為 16 以符合記憶體限制
#
# ============================================================================

# Data paths (WSL paths)
data:
  pure_dir: "/mnt/d/PURE"
  labels_dir: "/mnt/d/PURE_labels"
  wider_face_dir: "/mnt/d/WIDER_FACE/WIDER_train/images"
  wider_face_labels_dir: "/mnt/d/WIDER_FACE_labels"
  train_split: 0.8
  val_split: 0.2

# Model configuration
model:
  input_size: 256
  backbone: "mobilenetv2"
  backbone_alpha: 0.5  # Width multiplier for MobileNetV2
  num_landmarks: 5
  num_anchors: 6

# Training hyperparameters
training:
  batch_size: 16  # Reduced for ~80% GPU memory usage
  epochs: 100
  initial_learning_rate: 0.001
  min_learning_rate: 0.00001
  warmup_epochs: 5
  weight_decay: 0.0001
  num_workers: 8  # ~80% of 12 threads (i5-12400F)
  prefetch_buffer: 4  # Prefetch batches for efficiency

# Loss weights
loss:
  bbox_weight: 1.0
  landmark_weight: 0.5
  confidence_weight: 1.0

# Augmentation
augmentation:
  rotation_range: 30
  scale_range: [0.8, 1.2]
  brightness_range: 0.2
  contrast_range: 0.2
  blur_prob: 0.3
  blur_limit: 5
  noise_prob: 0.2
  horizontal_flip: true

# Anchors (normalized coordinates)
anchors:
  scales: [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]
  aspect_ratios: [1.0]

# Callbacks
callbacks:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  early_stopping_patience: 15
  reduce_lr_patience: 5
  reduce_lr_factor: 0.5

# Export settings
export:
  tflite_path: "export/face_detector.tflite"
  tfjs_path: "export/tfjs_model"
  quantize: true
